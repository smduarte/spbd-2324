{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2223/blob/main/lab1/SPBD_Labs_mapreduce1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKGXsDUIDxH6"
      },
      "source": [
        "# Python MapReduce Example\n",
        "\n",
        "Word count implemented in pure Python.\n",
        "\n",
        "This notebook exemplifies the execution of a map-reduce program in Python, using Hadoop.\n",
        "In this example, hadoop runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBR4IYlXDxH9"
      },
      "source": [
        "### Download the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "B68ZyGdPDxH9"
      },
      "outputs": [],
      "source": [
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8sllMoIDxH_"
      },
      "source": [
        "## WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two steps:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQd9aMLeDxH_"
      },
      "source": [
        "### Mapper\n",
        "\n",
        "By starting an element with \"%%file\", you are specifying that when run, the contents are written to the local disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbItx5zwDxIA"
      },
      "outputs": [],
      "source": [
        "%%file mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys\n",
        "import sys\n",
        "# import string library function  \n",
        "import string  \n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # remove punctuation characters\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "    print('words\\t%s' % len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1-nQlcDxIA"
      },
      "source": [
        "### Reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCDCm5cnDxIB"
      },
      "outputs": [],
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "total_count = 0\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    key, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    count = int(count)\n",
        "\n",
        "    total_count += count\n",
        "\n",
        "print('words\\t%s' % (total_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOA47uRDxIC"
      },
      "source": [
        "## Local execution\n",
        "\n",
        "The scripts can be tested using just the unix shell, as follows..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cORkS8pvDxIC"
      },
      "source": [
        "### Make the scripts executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuBS8D79DxID"
      },
      "outputs": [],
      "source": [
        "!chmod a+x mapper.py && chmod a+x reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dZTi7eIDxID"
      },
      "source": [
        "### Execute\n",
        "\n",
        "The execution workflow is as follows:\n",
        "\n",
        "+ The input file is piped into the input of the mapper;\n",
        "+ The output the mapper is sorted;\n",
        "+ The sorted output of the mapper is fed to the reducer stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZMaB-hDxIE"
      },
      "outputs": [],
      "source": [
        "!cat \"os_maias.txt\" | ./mapper.py | sort -k1,1 | ./reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MapReduce with HADOOP"
      ],
      "metadata": {
        "id": "rwuFmaseENu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Hadoop on Google Colab\n",
        "!curl -s https://raw.githubusercontent.com/smduarte/spbd-2223/main/lab1/install_hadoop.sh | bash"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GnLDSPxo4gKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC2bKZHtDxIE"
      },
      "source": [
        "## Hadoop standalone mode execution\n",
        "\n",
        "For executing in an hadoop cluster, input data should be moved into an HDFS directory. For executing in standalone mode, data can be read from the local filesystem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z1JJmqvDxIE"
      },
      "source": [
        "\n",
        "The output directory needs to be cleared..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCwnGGz7DxIF"
      },
      "outputs": [],
      "source": [
        "rm -rf results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xf3e1UGDxIF"
      },
      "source": [
        "### Submitting the job\n",
        "\n",
        "The _hadoop_ command is used to submit the mapreduce job to the cluster..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTxBwSF8DxIG"
      },
      "outputs": [],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-*streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input os_maias.txt -output results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzJQj5qDxIG"
      },
      "source": [
        "#### Checking the results\n",
        "The result is stored in directory results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Tow7rp4DxIG"
      },
      "outputs": [],
      "source": [
        "!cat results/part-*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}